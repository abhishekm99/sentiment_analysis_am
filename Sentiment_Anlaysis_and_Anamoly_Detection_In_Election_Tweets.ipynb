{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPG2HPR/Bop8BObj4uojjHQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishekm99/sentiment_analysis_am/blob/main/Sentiment_Anlaysis_and_Anamoly_Detection_In_Election_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    # Remove URLs\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet, flags=re.MULTILINE)\n",
        "    \n",
        "    # Remove special characters and numbers\n",
        "    tweet = re.sub(r\"[^\\w\\s]\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\d+\", \"\", tweet)\n",
        "    \n",
        "    # Convert to lowercase\n",
        "    tweet = tweet.lower()\n",
        "    \n",
        "    # Tokenize the tweet\n",
        "    tokens = word_tokenize(tweet)\n",
        "    \n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    \n",
        "    # Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    \n",
        "    # Join tokens back into a single string\n",
        "    processed_tweet = \" \".join(tokens)\n",
        "    \n",
        "    return processed_tweet\n"
      ],
      "metadata": {
        "id": "OO8YgfPtvP75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuXxmLFWtQ3d"
      },
      "outputs": [],
      "source": [
        "import tweepy\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Twitter API v2 credentials\n",
        "bearer_token = \"\",\n",
        "consumer_secret = \"\"\n",
        "consumer_key = \"\"\n",
        "access_token = \"\"\n",
        "access_token_secret = \"\"\n",
        "client = tweepy.Client(\n",
        "    bearer_token=bearer_token,\n",
        "    consumer_key=consumer_key, consumer_secret=consumer_secret,\n",
        "    access_token=access_token, access_token_secret=access_token_secret\n",
        ")\n",
        "\n",
        "# Collect tweets\n",
        "topic = \"Election\"  # Specify the topic you want to analyze\n",
        "tweet_count = 100  # Number of tweets to collect\n",
        "\n",
        "tweets = client.search_recent_tweets(query=topic, tweet_fields=[\"created_at\"], max_results=tweet_count, user_auth=True)\n",
        "\n",
        "# Perform sentiment analysis\n",
        "positive_tweets = 0\n",
        "negative_tweets = 0\n",
        "neutral_tweets = 0\n",
        "\n",
        "for tweet in tweets.data:\n",
        "    cleaned_tweet = preprocess_tweet(tweet.text)  # Implement the preprocess_tweet() function to clean the tweet text\n",
        "    \n",
        "    # Perform sentiment analysis using TextBlob\n",
        "    analysis = TextBlob(cleaned_tweet)\n",
        "    sentiment_score = analysis.sentiment.polarity\n",
        "    \n",
        "    if sentiment_score > 0:\n",
        "        positive_tweets += 1\n",
        "    elif sentiment_score < 0:\n",
        "        negative_tweets += 1\n",
        "    else:\n",
        "        neutral_tweets += 1\n",
        "\n",
        "# Calculate sentiment percentages\n",
        "total_tweets = positive_tweets + negative_tweets + neutral_tweets\n",
        "positive_percentage = (positive_tweets / total_tweets) * 100\n",
        "negative_percentage = (negative_tweets / total_tweets) * 100\n",
        "neutral_percentage = (neutral_tweets / total_tweets) * 100\n",
        "\n",
        "# Print sentiment distribution\n",
        "print(\"Sentiment Distribution:\")\n",
        "print(\"Positive: {:.2f}%\".format(positive_percentage))\n",
        "print(\"Negative: {:.2f}%\".format(negative_percentage))\n",
        "print(\"Neutral: {:.2f}%\".format(neutral_percentage))\n",
        "\n",
        "\n",
        "detect_anomaly(tweets)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_anomaly(tweets):\n",
        "    # Perform sentiment analysis and collect scores\n",
        "  sentiment_scores = []\n",
        "\n",
        "  for tweet in tweets.data:\n",
        "      cleaned_tweet = preprocess_tweet(tweet.text)  # Implement the preprocess_tweet() function to clean the tweet text\n",
        "      \n",
        "      # Perform sentiment analysis using TextBlob\n",
        "      analysis = TextBlob(cleaned_tweet)\n",
        "      sentiment_score = analysis.sentiment.polarity\n",
        "      sentiment_scores.append(sentiment_score)\n",
        "\n",
        "  # Convert sentiment scores to numpy array\n",
        "  sentiment_scores = np.array(sentiment_scores).reshape(-1, 1)\n",
        "\n",
        "  # Perform anomaly detection using Isolation Forest\n",
        "  isolation_forest = IsolationForest(contamination=0.1)\n",
        "  isolation_forest.fit(sentiment_scores)\n",
        "\n",
        "  # Predict anomalies\n",
        "  anomaly_predictions = isolation_forest.predict(sentiment_scores)\n",
        "\n",
        "  # Find the indices of anomalies\n",
        "  anomaly_indices = np.where(anomaly_predictions == -1)[0]\n",
        "\n",
        "  # Print the anomalous tweets\n",
        "  print(\"Anomalous Tweets:\")\n",
        "  for index in anomaly_indices:\n",
        "      print(tweets.data[index].text)\n",
        "      print(\"Sentiment Score:\", sentiment_scores[index][0])\n",
        "      print(\"-------------------------\")\n"
      ],
      "metadata": {
        "id": "nZOcJeqvuo8-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}